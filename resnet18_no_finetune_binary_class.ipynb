{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and settings\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe length: 19794\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>datetime</th>\n",
       "      <th>file_name</th>\n",
       "      <th>seq_id</th>\n",
       "      <th>frame_num</th>\n",
       "      <th>seq_num_frames</th>\n",
       "      <th>location</th>\n",
       "      <th>category_id</th>\n",
       "      <th>has_animal</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>N25_102EK113_04280033.JPG</td>\n",
       "      <td>2020-04-28 02:18:59+00:00</td>\n",
       "      <td>N25/102EK113/04280033.JPG</td>\n",
       "      <td>398e7044-7160-11ec-820f-5cf3706028c2</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>N25</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>collared_peccary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>N27_102EK113_06210753.JPG</td>\n",
       "      <td>2020-06-21 15:23:08+00:00</td>\n",
       "      <td>N27/102EK113/06210753.JPG</td>\n",
       "      <td>396b00d8-7160-11ec-b898-5cf3706028c2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>N27</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>black_agouti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>M01_100EK113_02180585.JPG</td>\n",
       "      <td>2020-02-18 18:12:48+00:00</td>\n",
       "      <td>M01/100EK113/02180585.JPG</td>\n",
       "      <td>39731938-7160-11ec-a7c8-5cf3706028c2</td>\n",
       "      <td>8</td>\n",
       "      <td>30</td>\n",
       "      <td>M01</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>black_agouti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>N09_100EK113_01120102.JPG</td>\n",
       "      <td>2020-01-12 12:39:51+00:00</td>\n",
       "      <td>N09/100EK113/01120102.JPG</td>\n",
       "      <td>39b31844-7160-11ec-be06-5cf3706028c2</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>N09</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M04_100EK113_01220825.JPG</td>\n",
       "      <td>2020-01-22 21:43:45+00:00</td>\n",
       "      <td>M04/100EK113/01220825.JPG</td>\n",
       "      <td>39a50b1b-7160-11ec-9853-5cf3706028c2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>M04</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>collared_peccary</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          id                   datetime  \\\n",
       "0  N25_102EK113_04280033.JPG  2020-04-28 02:18:59+00:00   \n",
       "1  N27_102EK113_06210753.JPG  2020-06-21 15:23:08+00:00   \n",
       "2  M01_100EK113_02180585.JPG  2020-02-18 18:12:48+00:00   \n",
       "3  N09_100EK113_01120102.JPG  2020-01-12 12:39:51+00:00   \n",
       "4  M04_100EK113_01220825.JPG  2020-01-22 21:43:45+00:00   \n",
       "\n",
       "                   file_name                                seq_id  frame_num  \\\n",
       "0  N25/102EK113/04280033.JPG  398e7044-7160-11ec-820f-5cf3706028c2          5   \n",
       "1  N27/102EK113/06210753.JPG  396b00d8-7160-11ec-b898-5cf3706028c2          2   \n",
       "2  M01/100EK113/02180585.JPG  39731938-7160-11ec-a7c8-5cf3706028c2          8   \n",
       "3  N09/100EK113/01120102.JPG  39b31844-7160-11ec-be06-5cf3706028c2          2   \n",
       "4  M04/100EK113/01220825.JPG  39a50b1b-7160-11ec-9853-5cf3706028c2          5   \n",
       "\n",
       "   seq_num_frames location  category_id  has_animal           species  \n",
       "0              12      N25            3           1  collared_peccary  \n",
       "1               3      N27            2           1      black_agouti  \n",
       "2              30      M01            2           1      black_agouti  \n",
       "3               6      N09            1           1             human  \n",
       "4               6      M04            3           1  collared_peccary  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load metadata CSV\n",
    "\n",
    "df = pd.read_csv(\"balanced_metadata.csv\")\n",
    "\n",
    "# Filter for images that contain an animal\n",
    "df = df[df[\"has_animal\"] == 1].reset_index(drop=True)\n",
    "\n",
    "image_path_prefix = \"Orinoquia_Carma_Traps/orinoquia_camera_traps_images/public/\"\n",
    "\n",
    "df = df.iloc[:, 1:]\n",
    "\n",
    "print(f\"Dataframe length: {len(df)}\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image tensor shape: (19794, 3, 224, 224)\n"
     ]
    }
   ],
   "source": [
    "# load image tensor as (N, C, H, W)\n",
    "# N: image count = len(df) = 19,794\n",
    "# C: channels = 3\n",
    "# H: height = 224\n",
    "# W: width = 224\n",
    "\n",
    "TARGET_SIZE = (224, 224)  # For ResNet18\n",
    "def load_image_for_resnet(full_path):\n",
    "    with Image.open(full_path) as im:\n",
    "        im = im.convert(\"RGB\").resize(TARGET_SIZE)\n",
    "        arr = np.asarray(im, dtype=np.uint8)\n",
    "\n",
    "        # Convert (H, W, C) to (C, H, W)\n",
    "        return np.transpose(arr, (2, 0, 1))\n",
    "\n",
    "paths = (os.path.join(image_path_prefix, p) for p in df[\"file_name\"])\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=os.cpu_count()) as exe:\n",
    "    X = np.asarray(list(exe.map(load_image_for_resnet, paths)))\n",
    "\n",
    "print(\"Image tensor shape:\", X.shape)  # Should be (N, 3, 224, 224)\n",
    "\n",
    "np.save(\"artifact/orinoquia_resnet18_imageset.npy\", X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if artifact is saved\n",
    "X = np.load(\"artifact/orinoquia_resnet18_imageset.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 20,  20,  20, ...,  19,  19,  18],\n",
       "         [ 20,  20,  20, ...,  18,  19,  19],\n",
       "         [ 20,  20,  20, ...,  20,  19,  20],\n",
       "         ...,\n",
       "         [184, 211, 215, ..., 255, 255, 255],\n",
       "         [184, 211, 215, ..., 255, 255, 255],\n",
       "         [180, 209, 215, ..., 235, 234, 233]],\n",
       "\n",
       "        [[ 20,  20,  20, ...,  19,  19,  18],\n",
       "         [ 20,  20,  20, ...,  18,  19,  19],\n",
       "         [ 20,  20,  20, ...,  20,  19,  20],\n",
       "         ...,\n",
       "         [106,  98,  92, ..., 251, 251, 251],\n",
       "         [106,  98,  93, ..., 253, 253, 253],\n",
       "         [101,  97,  93, ..., 231, 230, 230]],\n",
       "\n",
       "        [[ 20,  20,  20, ...,  19,  19,  18],\n",
       "         [ 20,  20,  20, ...,  18,  19,  19],\n",
       "         [ 20,  20,  20, ...,  20,  19,  20],\n",
       "         ...,\n",
       "         [ 76,  54,  43, ..., 255, 255, 255],\n",
       "         [ 77,  54,  43, ..., 255, 255, 255],\n",
       "         [ 71,  53,  43, ..., 235, 234, 234]]],\n",
       "\n",
       "\n",
       "       [[[ 48,  87, 126, ..., 104, 148,  76],\n",
       "         [ 39,  77, 104, ...,  99,  64,  40],\n",
       "         [ 49,  70,  91, ...,  86, 114,  44],\n",
       "         ...,\n",
       "         [184, 211, 215, ..., 255, 255, 255],\n",
       "         [184, 211, 215, ..., 255, 255, 255],\n",
       "         [179, 209, 215, ..., 232, 236, 233]],\n",
       "\n",
       "        [[ 41,  77, 118, ..., 139, 193, 104],\n",
       "         [ 37,  67,  90, ..., 130,  94,  57],\n",
       "         [ 47,  59,  78, ...,  99, 128,  53],\n",
       "         ...,\n",
       "         [106,  98,  92, ..., 251, 251, 251],\n",
       "         [106,  98,  93, ..., 253, 253, 253],\n",
       "         [100,  97,  93, ..., 229, 234, 231]],\n",
       "\n",
       "        [[ 41,  65, 101, ...,  67, 121,  87],\n",
       "         [ 37,  59,  79, ...,  77,  81,  67],\n",
       "         [ 58,  56,  65, ..., 111, 154,  76],\n",
       "         ...,\n",
       "         [ 76,  54,  43, ..., 255, 255, 255],\n",
       "         [ 76,  54,  43, ..., 255, 255, 255],\n",
       "         [ 71,  53,  43, ..., 231, 235, 233]]],\n",
       "\n",
       "\n",
       "       [[[ 31,  27,  27, ...,  47,  45,  49],\n",
       "         [ 32,  27,  28, ...,  36,  38,  35],\n",
       "         [ 30,  26,  29, ...,  42,  45,  46],\n",
       "         ...,\n",
       "         [184, 211, 215, ..., 255, 255, 255],\n",
       "         [184, 211, 215, ..., 255, 255, 255],\n",
       "         [180, 209, 215, ..., 231, 231, 231]],\n",
       "\n",
       "        [[ 31,  27,  27, ...,  47,  45,  49],\n",
       "         [ 32,  27,  28, ...,  36,  38,  35],\n",
       "         [ 30,  26,  29, ...,  42,  45,  46],\n",
       "         ...,\n",
       "         [106,  98,  92, ..., 251, 251, 251],\n",
       "         [106,  98,  93, ..., 253, 253, 253],\n",
       "         [101,  97,  92, ..., 227, 227, 227]],\n",
       "\n",
       "        [[ 31,  27,  27, ...,  47,  45,  49],\n",
       "         [ 32,  27,  28, ...,  36,  38,  35],\n",
       "         [ 30,  26,  29, ...,  42,  45,  46],\n",
       "         ...,\n",
       "         [ 76,  54,  43, ..., 255, 255, 255],\n",
       "         [ 76,  54,  43, ..., 255, 255, 255],\n",
       "         [ 71,  53,  43, ..., 231, 232, 231]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[ 41,  40,  37, ...,  71,  98,  90],\n",
       "         [ 32,  42,  37, ...,  72,  71,  86],\n",
       "         [119,  54,  28, ...,  45,  55,  46],\n",
       "         ...,\n",
       "         [184, 211, 215, ..., 255, 255, 255],\n",
       "         [184, 211, 215, ..., 255, 255, 255],\n",
       "         [180, 209, 215, ..., 236, 237, 236]],\n",
       "\n",
       "        [[ 73,  71,  67, ...,  83, 118, 101],\n",
       "         [ 67,  73,  65, ...,  75,  79,  87],\n",
       "         [156,  89,  57, ...,  45,  58,  46],\n",
       "         ...,\n",
       "         [106,  98,  92, ..., 251, 251, 251],\n",
       "         [106,  98,  93, ..., 253, 253, 253],\n",
       "         [101,  97,  93, ..., 233, 233, 231]],\n",
       "\n",
       "        [[ 34,  35,  49, ...,  57, 112, 110],\n",
       "         [ 46,  43,  47, ...,  68,  82,  91],\n",
       "         [147,  78,  52, ...,  39,  57,  49],\n",
       "         ...,\n",
       "         [ 76,  54,  43, ..., 255, 255, 255],\n",
       "         [ 76,  54,  43, ..., 255, 255, 255],\n",
       "         [ 71,  53,  43, ..., 235, 235, 233]]],\n",
       "\n",
       "\n",
       "       [[[142, 147, 153, ..., 167, 162, 164],\n",
       "         [149, 150, 154, ..., 174, 153, 154],\n",
       "         [154, 147, 153, ..., 143, 127, 129],\n",
       "         ...,\n",
       "         [184, 211, 215, ..., 255, 255, 255],\n",
       "         [184, 211, 215, ..., 255, 255, 255],\n",
       "         [182, 210, 215, ..., 249, 246, 247]],\n",
       "\n",
       "        [[142, 147, 153, ..., 167, 162, 164],\n",
       "         [149, 150, 154, ..., 174, 153, 154],\n",
       "         [154, 147, 153, ..., 143, 127, 129],\n",
       "         ...,\n",
       "         [106,  98,  92, ..., 251, 251, 251],\n",
       "         [106,  98,  93, ..., 252, 252, 252],\n",
       "         [104,  98,  93, ..., 245, 242, 243]],\n",
       "\n",
       "        [[142, 147, 153, ..., 167, 162, 164],\n",
       "         [149, 150, 154, ..., 174, 153, 154],\n",
       "         [154, 147, 153, ..., 143, 127, 129],\n",
       "         ...,\n",
       "         [ 76,  54,  43, ..., 255, 255, 255],\n",
       "         [ 76,  54,  43, ..., 255, 255, 255],\n",
       "         [ 74,  54,  43, ..., 249, 246, 247]]],\n",
       "\n",
       "\n",
       "       [[[ 20,  20,  21, ...,  25,  22,  21],\n",
       "         [ 20,  20,  21, ...,  26,  22,  21],\n",
       "         [ 21,  21,  21, ...,  26,  23,  22],\n",
       "         ...,\n",
       "         [184, 211, 215, ..., 255, 255, 255],\n",
       "         [184, 211, 215, ..., 255, 255, 255],\n",
       "         [181, 209, 215, ..., 230, 230, 230]],\n",
       "\n",
       "        [[ 20,  20,  21, ...,  25,  22,  21],\n",
       "         [ 20,  20,  21, ...,  26,  22,  21],\n",
       "         [ 21,  21,  21, ...,  26,  23,  22],\n",
       "         ...,\n",
       "         [106,  98,  92, ..., 251, 251, 251],\n",
       "         [106,  98,  93, ..., 253, 253, 253],\n",
       "         [102,  97,  92, ..., 226, 226, 226]],\n",
       "\n",
       "        [[ 20,  20,  21, ...,  25,  22,  21],\n",
       "         [ 20,  20,  21, ...,  26,  22,  21],\n",
       "         [ 21,  21,  21, ...,  26,  23,  22],\n",
       "         ...,\n",
       "         [ 76,  54,  43, ..., 255, 255, 255],\n",
       "         [ 76,  54,  43, ..., 255, 255, 255],\n",
       "         [ 72,  53,  43, ..., 230, 230, 230]]]],\n",
       "      shape=(19794, 3, 224, 224), dtype=uint8)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size: 17814\n",
      "Validation size: 1980\n"
     ]
    }
   ],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from torchvision import transforms as T\n",
    "\n",
    "# ---------- helpers ----------------------------------------------------------\n",
    "# always-present tail of the pipeline\n",
    "base_transform = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406],  # ResNet-18 values\n",
    "                std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "def merge_transforms(augment: T.Compose | None = None) -> T.Compose:\n",
    "    \"\"\"\n",
    "    Return a Compose that is `augment` followed by the common base_transform.\n",
    "    If `augment` is None, you just get the base_transform.\n",
    "    \"\"\"\n",
    "    if augment is None:\n",
    "        return base_transform\n",
    "    return T.Compose(augment.transforms + base_transform.transforms)\n",
    "\n",
    "\n",
    "# ---------- dataset ----------------------------------------------------------\n",
    "class CameraTrapDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        images: np.ndarray,           # shape (N, 3, 224, 224)\n",
    "        index: pd.Index,\n",
    "        transform: T.Compose | None = None,\n",
    "    ):\n",
    "        self.df = df\n",
    "        self.images = images\n",
    "        self.index = index\n",
    "        self.transform = merge_transforms(transform) if transform is not None else base_transform\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.index)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        row_idx = self.index[idx]\n",
    "        row = self.df.iloc[row_idx]\n",
    "\n",
    "        img = self.images[row_idx] # ndarray (3, 224, 224)\n",
    "        img = T.ToPILImage()(img.transpose((1, 2, 0)))\n",
    "        img = self.transform(img) # tensor (3, 224, 224)\n",
    "\n",
    "        label = torch.tensor(row[\"has_animal\"], dtype=torch.float32) # ()\n",
    "\n",
    "        return img, label\n",
    "\n",
    "def make_train_val_index(\n",
    "    df: pd.DataFrame, train_ratio: float = 0.9\n",
    ") -> Tuple[pd.Index, pd.Index]:\n",
    "    shuffled = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    split = int(train_ratio * len(shuffled))\n",
    "    return shuffled.index[:split], shuffled.index[split:]\n",
    "\n",
    "train_idx, val_idx = make_train_val_index(df)\n",
    "\n",
    "train_ds = CameraTrapDataset(df, X, train_idx, transform=T.Compose([\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.RandomRotation(10),\n",
    "]))\n",
    "\n",
    "val_ds = CameraTrapDataset(df, X, val_idx, transform=None)\n",
    "\n",
    "print(f\"Training size: {len(train_ds)}\")\n",
    "print(f\"Validation size: {len(val_ds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single image load time: 0.002100229263305664\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "t0 = time()\n",
    "img, label = train_ds[0]\n",
    "print(\"Single image load time:\", time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 224, 224]) torch.Size([32]) tensor([1.])\n"
     ]
    }
   ],
   "source": [
    "train_dl = DataLoader(train_ds, batch_size=32, shuffle=True, num_workers=0)\n",
    "val_dl = DataLoader(val_ds, batch_size=32, shuffle=False, num_workers=0)\n",
    "\n",
    "imgs, labels = next(iter(train_dl))\n",
    "\n",
    "# imgs (BATCH_SIZE, 3, 224, 224)\n",
    "# labels (BATCH_SIZE)\n",
    "# 0 = no animal, 1 = animal\n",
    "print(imgs.shape, labels.shape, labels.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Device cuda\n"
     ]
    }
   ],
   "source": [
    "from torchvision import models\n",
    "from tqdm import tqdm\n",
    "\n",
    "LR = 1e-4\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Using Device {DEVICE}\")\n",
    "\n",
    "resnet = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "\n",
    "# freeze all layers\n",
    "for p in resnet.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "in_features = resnet.fc.in_features\n",
    "resnet.fc = nn.Linear(in_features, 1)\n",
    "resnet = resnet.to(DEVICE)\n",
    "\n",
    "optimizer = optim.Adam(resnet.fc.parameters(), lr=LR)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "@torch.no_grad()\n",
    "def run_eval_epoch(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "    for imgs, labels in tqdm(dataloader, desc=\"Val  \", leave=False):\n",
    "        imgs   = imgs.to(DEVICE)\n",
    "        labels = labels.to(DEVICE).unsqueeze(1)\n",
    "\n",
    "        logits = model(imgs)\n",
    "        loss   = criterion(logits, labels)\n",
    "\n",
    "        running_loss += loss.item() * imgs.size(0)\n",
    "        preds = (logits.sigmoid() >= 0.5).int()\n",
    "        correct += (preds == labels.int()).sum().item()\n",
    "        total   += imgs.size(0)\n",
    "\n",
    "    return running_loss / total, correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01/20] train loss 0.0520  acc 0.996 | val loss 0.0054  acc 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02/20] train loss 0.0030  acc 1.000 | val loss 0.0018  acc 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03/20] train loss 0.0013  acc 1.000 | val loss 0.0009  acc 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04/20] train loss 0.0007  acc 1.000 | val loss 0.0006  acc 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05/20] train loss 0.0005  acc 1.000 | val loss 0.0004  acc 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                        \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     16\u001b[39m running_loss, correct, seen = \u001b[32m0.0\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m\n\u001b[32m     18\u001b[39m pbar = tqdm(train_dl, desc=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m, leave=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpbar\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mglobal_step\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m   \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\conta\\Documents\\GitHub\\camera-trap-image-classification\\.venv\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\conta\\Documents\\GitHub\\camera-trap-image-classification\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    698\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    699\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    700\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m701\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    702\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    703\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    704\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    705\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    706\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    707\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\conta\\Documents\\GitHub\\camera-trap-image-classification\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    755\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    756\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m757\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    758\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    759\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\conta\\Documents\\GitHub\\camera-trap-image-classification\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 50\u001b[39m, in \u001b[36mCameraTrapDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     47\u001b[39m row = \u001b[38;5;28mself\u001b[39m.df.iloc[row_idx]\n\u001b[32m     49\u001b[39m img = \u001b[38;5;28mself\u001b[39m.images[row_idx] \u001b[38;5;66;03m# ndarray (3, 224, 224)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m img = \u001b[43mT\u001b[49m\u001b[43m.\u001b[49m\u001b[43mToPILImage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m img = \u001b[38;5;28mself\u001b[39m.transform(img) \u001b[38;5;66;03m# tensor (3, 224, 224)\u001b[39;00m\n\u001b[32m     53\u001b[39m label = torch.tensor(row[\u001b[33m\"\u001b[39m\u001b[33mhas_animal\u001b[39m\u001b[33m\"\u001b[39m], dtype=torch.float32) \u001b[38;5;66;03m# ()\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\conta\\Documents\\GitHub\\camera-trap-image-classification\\.venv\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:234\u001b[39m, in \u001b[36mToPILImage.__call__\u001b[39m\u001b[34m(self, pic)\u001b[39m\n\u001b[32m    225\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[32m    226\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    227\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m    228\u001b[39m \u001b[33;03m        pic (Tensor or numpy.ndarray): Image to be converted to PIL Image.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    232\u001b[39m \n\u001b[32m    233\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m234\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_pil_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\conta\\Documents\\GitHub\\camera-trap-image-classification\\.venv\\Lib\\site-packages\\torchvision\\transforms\\functional.py:324\u001b[39m, in \u001b[36mto_pil_image\u001b[39m\u001b[34m(pic, mode)\u001b[39m\n\u001b[32m    321\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    322\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInput type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnpimg.dtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is not supported\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m324\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mImage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfromarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnpimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\conta\\Documents\\GitHub\\camera-trap-image-classification\\.venv\\Lib\\site-packages\\PIL\\Image.py:3326\u001b[39m, in \u001b[36mfromarray\u001b[39m\u001b[34m(obj, mode)\u001b[39m\n\u001b[32m   3323\u001b[39m         msg = \u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[33mstrides\u001b[39m\u001b[33m'\u001b[39m\u001b[33m requires either tobytes() or tostring()\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3324\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m-> \u001b[39m\u001b[32m3326\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfrombuffer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mraw\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrawmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\conta\\Documents\\GitHub\\camera-trap-image-classification\\.venv\\Lib\\site-packages\\PIL\\Image.py:3216\u001b[39m, in \u001b[36mfrombuffer\u001b[39m\u001b[34m(mode, size, data, decoder_name, *args)\u001b[39m\n\u001b[32m   3213\u001b[39m         im.readonly = \u001b[32m1\u001b[39m\n\u001b[32m   3214\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m im\n\u001b[32m-> \u001b[39m\u001b[32m3216\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfrombytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\conta\\Documents\\GitHub\\camera-trap-image-classification\\.venv\\Lib\\site-packages\\PIL\\Image.py:3153\u001b[39m, in \u001b[36mfrombytes\u001b[39m\u001b[34m(mode, size, data, decoder_name, *args)\u001b[39m\n\u001b[32m   3150\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m decoder_name == \u001b[33m\"\u001b[39m\u001b[33mraw\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m decoder_args == ():\n\u001b[32m   3151\u001b[39m         decoder_args = mode\n\u001b[32m-> \u001b[39m\u001b[32m3153\u001b[39m     \u001b[43mim\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrombytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3154\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m im\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\conta\\Documents\\GitHub\\camera-trap-image-classification\\.venv\\Lib\\site-packages\\PIL\\Image.py:871\u001b[39m, in \u001b[36mImage.frombytes\u001b[39m\u001b[34m(self, data, decoder_name, *args)\u001b[39m\n\u001b[32m    869\u001b[39m d = _getdecoder(\u001b[38;5;28mself\u001b[39m.mode, decoder_name, decoder_args)\n\u001b[32m    870\u001b[39m d.setimage(\u001b[38;5;28mself\u001b[39m.im)\n\u001b[32m--> \u001b[39m\u001b[32m871\u001b[39m s = \u001b[43md\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    873\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m s[\u001b[32m0\u001b[39m] >= \u001b[32m0\u001b[39m:\n\u001b[32m    874\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mnot enough image data\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "EVAL_EVERY = 100          # batches\n",
    "NUM_EPOCHS = 20\n",
    "\n",
    "history = {\n",
    "    \"step\":        [],\n",
    "    \"epoch\":       [],\n",
    "    \"train_loss\":  [],\n",
    "    \"val_loss\":    [],\n",
    "    \"train_acc\":   [],\n",
    "    \"val_acc\":     [],\n",
    "}\n",
    "\n",
    "global_step = 0\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    resnet.train()\n",
    "    running_loss, correct, seen = 0.0, 0, 0\n",
    "\n",
    "    pbar = tqdm(train_dl, desc=f\"Epoch {epoch}\", leave=False)\n",
    "    for imgs, labels in pbar:\n",
    "        global_step += 1\n",
    "\n",
    "        imgs   = imgs.to(DEVICE)\n",
    "        labels = labels.to(DEVICE).unsqueeze(1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = resnet(imgs)\n",
    "        loss   = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * imgs.size(0)\n",
    "        preds   = (logits.sigmoid() >= 0.5).int()\n",
    "        correct += (preds == labels.int()).sum().item()\n",
    "        seen    += imgs.size(0)\n",
    "\n",
    "        if global_step % EVAL_EVERY == 0:\n",
    "            train_loss = running_loss / seen\n",
    "            train_acc  = correct / seen\n",
    "\n",
    "            val_loss, val_acc = run_eval_epoch(\n",
    "                model=resnet,\n",
    "                dataloader=val_dl,\n",
    "                criterion=criterion,\n",
    "            )\n",
    "\n",
    "            history[\"step\"].append(global_step)\n",
    "            history[\"epoch\"].append((epoch - 1) + seen / len(train_dl.dataset))\n",
    "            history[\"train_loss\"].append(train_loss)\n",
    "            history[\"val_loss\"].append(val_loss)\n",
    "            history[\"train_acc\"].append(train_acc)\n",
    "            history[\"val_acc\"].append(val_acc)\n",
    "\n",
    "            pbar.set_postfix({\n",
    "                \"tr_loss\": f\"{train_loss:.3f}\",\n",
    "                \"val_loss\": f\"{val_loss:.3f}\",\n",
    "                \"val_acc\": f\"{val_acc:.3f}\",\n",
    "            })\n",
    "\n",
    "    val_loss, val_acc = run_eval_epoch(\n",
    "        model=resnet,\n",
    "        dataloader=val_dl,\n",
    "        criterion=criterion,\n",
    "    )\n",
    "    train_loss = running_loss / seen\n",
    "    train_acc  = correct / seen\n",
    "\n",
    "    history[\"step\"].append(global_step)\n",
    "    history[\"epoch\"].append(epoch)\n",
    "    history[\"train_loss\"].append(train_loss)\n",
    "    history[\"val_loss\"].append(val_loss)\n",
    "    history[\"train_acc\"].append(train_acc)\n",
    "    history[\"val_acc\"].append(val_acc)\n",
    "\n",
    "    print(f\"[{epoch:02}/{NUM_EPOCHS}] \"\n",
    "          f\"train loss {train_loss:.4f}  acc {train_acc:.3f} | \"\n",
    "          f\"val loss {val_loss:.4f}  acc {val_acc:.3f}\")\n",
    "\n",
    "df_history = pd.DataFrame(history)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(df_history[\"step\"], df_history[\"train_loss\"], label=\"Train Loss\")\n",
    "plt.plot(df_history[\"step\"], df_history[\"val_loss\"],   label=\"Val Loss\")\n",
    "plt.xlabel(\"Global step\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training / Validation loss vs. step\")\n",
    "plt.legend(); plt.grid(True); plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
